@article{https://doi.org/10.1002/adts.202300378,
author = {Moayed Baharlou, Sina and Hemayat, Saeed and Toussaint Jr., Kimani C. and Ndao, Abdoulaye},
title = {GPU-Accelerated and Memory-Independent Layout Generation for Arbitrarily Large-Scale Metadevices},
journal = {Advanced Theory and Simulations},
volume = {n/a},
number = {n/a},
pages = {2300378},
keywords = {GDSII, GPU, layout, metadevices, parallel},
doi = {https://doi.org/10.1002/adts.202300378},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/adts.202300378},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/adts.202300378},
abstract = {Abstract Metadevices are of paramount interest and importance due to their exotic capabilities in light control and manipulation in the nano-scale regime. Various practical applications, including wearable devices, lasers, optical sensors, high-resolution microscopy, both virtual and augmented reality, and metasails, require large-scale metasurfaces. However, layout generation for large metasurfaces, faces challenges as up to billions of unit cells are required to create a metasurface, resulting in slow speeds and high memory demands. Here, a new framework namely ParallelGDS is proposed for fully parallel, ultra-fast, and memory-independent generation of graphic design system (GDSII) files for arbitrarily large metasurfaces. Compared to existing methods such as GDSTk, GDSPy, LUMERICAL's polystencil, the proposed framework significantly accelerates the layout generation process by factors of 10 to 100. More importantly, ParallelGDS drastically reduces the required memory (an O(n2)\$\mathcal {O}(n^{2})\$ problem) with an average reduction factor of 0.5×Dn2\$0.5 \times D^{2}\_{\text{n}}\$ where Dn\$D\_{\text{n}}\$ is the normalized metasurface diameter, as the framework uses only ≈2 GB of memory regardless of the size of the metasurface. The proposed framework offers complete control over memory requirements and parallelization levels of layout generation, ranging from single-core CPU usage to multithreaded CPU utilization, and finally, full utilization of all GPU cores.}
}

